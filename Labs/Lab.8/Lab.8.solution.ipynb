{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for SUSY Dataset\n",
    "\n",
    "Use the SUSY dataset for the rest of this lab. Here is a basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz > SUSY.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gunzip\n\u001b[0;32m      2\u001b[0m gunzip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUSYSMALL.csv.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sh'"
     ]
    }
   ],
   "source": [
    "from sh import gunzip\n",
    "gunzip(\"SUSYSMALL.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our usual libraries...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"SUSY-small.csv\"\n",
    "filename=\"../Lab.7/SUSY.csv\"\n",
    "VarNames=[\"signal\", \"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \n",
    "          \"l_2_phi\", \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\",\n",
    "          \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]\n",
    "df = pd.read_csv(filename, dtype='float64', names=VarNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn\n",
    "\n",
    "[Scikit-learn](http://scikit-learn.org) is a rich python library for data science, including machine learning. For example, we can build a Fisher Discriminant (aka Linear Discriminant Analysis, or LDA). \n",
    "\n",
    "### Exercise 1: Install Scikit-Learn\n",
    "\n",
    "Follow the [Installation Instructions](https://scikit-learn.org/stable/install.html) and install `scikit-learn` in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Read About Classifiers\n",
    "\n",
    "#### Part a\n",
    "Scikit-learn offers an impressively comprehensive list of machine learning algorithms. Browse through [scikit-learn's documentation](https://scikit-learn.org/stable/index.html). You'll note the algorithms are organized into classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. Browse through the list of [classification algorithms](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). \n",
    "\n",
    "#### Part b\n",
    "Note scikit-learn's documentation is rather comprehensive. The documentation on [linear models](https://scikit-learn.org/stable/modules/linear_model.html) shows how classification problems are setup. Read about the first few methods and try to comprehend the example codes. Skim the rest of the document.\n",
    "\n",
    "#### Part c\n",
    "Read through the [LDA Documentation](https://scikit-learn.org/stable/modules/lda_qda.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Training a Classifier\n",
    "\n",
    "Lets' repeat what we did manually in the previous lab using scikit-learn. We'll use a LDA classifier, which we can instanciate as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.discriminant_analysis as DA\n",
    "Fisher=DA.LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, to properly formulate our problem, we'll have to:\n",
    "\n",
    "* Define the inputs (X) vs outputs (Y)\n",
    "* Designate training vs testing samples (in order to get a unbias assessment of the performance of Machine Learning algorithms)\n",
    "\n",
    "for example, here we'll take use 4M events for training and the remainder for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Train=400\n",
    "\n",
    "Train_Sample=df[:N_Train]\n",
    "Test_Sample=df[N_Train:]\n",
    "\n",
    "X_Train=Train_Sample[VarNames[1:]]\n",
    "y_Train=Train_Sample[\"signal\"]\n",
    "\n",
    "X_Test=Test_Sample[VarNames[1:]]\n",
    "y_Test=Test_Sample[\"signal\"]\n",
    "\n",
    "Test_sig=Test_Sample[Test_Sample.signal==1]\n",
    "Test_bkg=Test_Sample[Test_Sample.signal==0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the classifier as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 18)) while a minimum of 2 is required by LinearDiscriminantAnalysis.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Fisher\u001b[38;5;241m.\u001b[39mfit(X_Train,y_Train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:575\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    573\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 575\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    576\u001b[0m     X, y, ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m[xp\u001b[38;5;241m.\u001b[39mfloat64, xp\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m    577\u001b[0m )\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m unique_labels(y)\n\u001b[0;32m    579\u001b[0m n_samples, _ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 18)) while a minimum of 2 is required by LinearDiscriminantAnalysis."
     ]
    }
   ],
   "source": [
    "Fisher.fit(X_Train,y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the output, comparing signal and background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 18)) while a minimum of 1 is required by LinearDiscriminantAnalysis.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(Fisher\u001b[38;5;241m.\u001b[39mdecision_function(Test_sig[VarNames[\u001b[38;5;241m1\u001b[39m:]]),bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,histtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m,stacked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(Fisher\u001b[38;5;241m.\u001b[39mdecision_function(Test_bkg[VarNames[\u001b[38;5;241m1\u001b[39m:]]),bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,histtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m,stacked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper right\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:746\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply decision function to an array of samples.\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03mThe decision function is equal (up to a constant factor) to the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;124;03m    log likelihood ratio of the positive class.\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# Only override for the doc\u001b[39;00m\n\u001b[1;32m--> 746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    401\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 18)) while a minimum of 1 is required by LinearDiscriminantAnalysis."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(Fisher.decision_function(Test_sig[VarNames[1:]]),bins=100,histtype=\"step\", color=\"blue\", label=\"signal\",stacked=True)\n",
    "plt.hist(Fisher.decision_function(Test_bkg[VarNames[1:]]),bins=100,histtype=\"step\", color=\"red\", label=\"background\",stacked=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a\n",
    "\n",
    "Compare ROC curves computed on the test versus training samples, in a single plot. Do you see a bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "train_data = df[:N_Train]\n",
    "test_data = df[N_Train:]\n",
    "\n",
    "X_train = train_data[VarNames[1:]]\n",
    "y_train = train_data[\"signal\"]\n",
    "\n",
    "X_test = test_data[VarNames[1:]]\n",
    "y_test = test_data[\"signal\"]\n",
    "\n",
    "lda_classifier = DA.LinearDiscriminantAnalysis()\n",
    "lda_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for train and test datasets\n",
    "train_probs = lda_classifier.predict_proba(X_train)[:, 1]\n",
    "test_probs = lda_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC and AUC for train and test sets\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, train_probs)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, test_probs)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_test, tpr_test, color='blue', lw=2, label=f'Test ROC (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot(fpr_train, tpr_train, color='black', lw=2, label=f'Train ROC (AUC = {roc_auc_train:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--')  # Diagonal (random guess)\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curves for Train and Test Samples')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Train the Fisher performance of using the raw, features, and raw+features as input. Compare the performance one a single plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = [\"l_1_pT\", \"l_1_eta\", \"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\"]\n",
    "derived_features = list(set(VarNames[1:]).difference(raw_features))\n",
    "\n",
    "X_raw = df[raw_features]\n",
    "X_derived = df[derived_features]\n",
    "X_all_features = df[VarNames[1:]]\n",
    "y_all_features = df[\"signal\"]\n",
    "\n",
    "# Initializing classifiers for each feature set\n",
    "lda_raw = DA.LinearDiscriminantAnalysis()\n",
    "lda_derived = DA.LinearDiscriminantAnalysis()\n",
    "lda_combined = DA.LinearDiscriminantAnalysis()\n",
    "\n",
    "lda_raw.fit(X_raw, y_all_features)\n",
    "lda_derived.fit(X_derived, y_all_features)\n",
    "lda_combined.fit(X_all_features, y_all_features)\n",
    "\n",
    "# Predicting probabilities for each set\n",
    "raw_probs = lda_raw.predict_proba(X_raw)[:, 1]\n",
    "derived_probs = lda_derived.predict_proba(X_derived)[:, 1]\n",
    "combined_probs = lda_combined.predict_proba(X_all_features)[:, 1]\n",
    "\n",
    "# Compute ROC and AUC for each set\n",
    "fpr_raw, tpr_raw, _ = roc_curve(y_all_features, raw_probs)\n",
    "roc_auc_raw = auc(fpr_raw, tpr_raw)\n",
    "\n",
    "fpr_derived, tpr_derived, _ = roc_curve(y_all_features, derived_probs)\n",
    "roc_auc_derived = auc(fpr_derived, tpr_derived)\n",
    "\n",
    "fpr_combined, tpr_combined, _ = roc_curve(y_all_features, combined_probs)\n",
    "roc_auc_combined = auc(fpr_combined, tpr_combined)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_raw, tpr_raw, color='purple', lw=2, label=f'Raw Features ROC (AUC = {roc_auc_raw:.2f})')\n",
    "plt.plot(fpr_derived, tpr_derived, color='blue', lw=2, label=f'Derived Features ROC (AUC = {roc_auc_derived:.2f})')\n",
    "plt.plot(fpr_combined, tpr_combined, color='green', lw=2, label=f'Combined Features ROC (AUC = {roc_auc_combined:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--')  # Diagonal (random guess)\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curves for Raw, Derived, and Combined Features')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Comparing Techniques\n",
    "\n",
    "#### Part a\n",
    "Select 3 different classifiers from the techniques listed [here](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) to compare. Note that you can use the multi-layer perceptron to build a deep network, though training may be prohibitively slow. So avoid this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "knn_model = KNeighborsClassifier()\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Use a smaller subset of data for quicker execution\n",
    "!head -500000 SUSY.csv > SUSY-small.csv\n",
    "small_dataset = pd.read_csv(\"SUSY-small.csv\", dtype='float64', names=VarNames)\n",
    "\n",
    "# Splitting the smaller dataset: 80% train, 20% test\n",
    "train_data = small_dataset[100000:]\n",
    "test_data = small_dataset[:100000]\n",
    "\n",
    "X_train = train_data[VarNames[1:]]\n",
    "y_train = train_data[\"signal\"]\n",
    "\n",
    "X_test = test_data[VarNames[1:]]\n",
    "y_test = test_data[\"signal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Write a function that takes an instantiated classifier and performs the comparison from part 3b. Use the function on your choice of functions in part a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classifier_performance(classifier, X_raw, X_features, X_combined, y_train):\n",
    "    classifier.fit(X_raw, y_train)\n",
    "    raw_importances = classifier.coef_ if hasattr(classifier, 'coef_') else classifier.feature_importances_\n",
    "\n",
    "    classifier.fit(X_features, y_train)\n",
    "    feature_importances = classifier.coef_ if hasattr(classifier, 'coef_') else classifier.feature_importances_\n",
    "\n",
    "    classifier.fit(X_combined, y_train)\n",
    "    combined_importances = classifier.coef_ if hasattr(classifier, 'coef_') else classifier.feature_importances_\n",
    "\n",
    "    fisher_scores_raw = np.abs(raw_importances)\n",
    "    fisher_scores_features = np.abs(feature_importances)\n",
    "    fisher_scores_combined = np.abs(combined_importances)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fisher_scores_raw, label='Raw Data', marker='o')\n",
    "    plt.plot(fisher_scores_features, label='Features', marker='o')\n",
    "    plt.plot(fisher_scores_combined, label='Raw + Features', marker='o')\n",
    "\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Fisher Score')\n",
    "    plt.title('Fisher Performance Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "compare_classifier_performance(svm_model, X_raw, X_features, X_combined, y_train)\n",
    "compare_classifier_performance(knn_model, X_raw, X_features, X_combined, y_train)\n",
    "compare_classifier_performance(dt_model, X_raw, X_features, X_combined, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "\n",
    "Use the best method from part c to compute the maximal significance $\\sigma_S= \\frac{N_S}{\\sqrt{N_S+N_B}}$ for the scenarios in lab 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = dt_model.predict(X_test)\n",
    "\n",
    "signal_count = np.sum(predictions == 1)\n",
    "background_count = np.sum(predictions == 0)\n",
    "\n",
    "max_significance = signal_count / np.sqrt(signal_count + background_count)\n",
    "\n",
    "print(\"Maximal Significance (Ïƒ_S):\", max_significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Metrics\n",
    "\n",
    "Scikit-learn provides methods for computing the FPR, TPR, ROC, AUC metrics. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_Test, Fisher.decision_function(X_Test))\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr,tpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part a\n",
    "TPR/FPR/ROC/AUC are one way of assessing the quality of a classifier. Read about [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall), [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision), and [F-score](https://en.wikipedia.org/wiki/F-score).\n",
    "\n",
    "#### Part b\n",
    "Look through [model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#) documentation. Using scikit-learns tools, compute TPR, FPR, ROC, AUC, Precision, Recall, F1 score, and accuracy for the method you selected in 4c above and each scenario. Make a nice table, which also includes the maximal significance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, \\\n",
    "                            precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def compute_classifier_metrics(classifier, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "    # Find threshold for maximum F1 score\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.unique(y_prob):\n",
    "        y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "        current_f1 = f1_score(y_test, y_pred_thresh)\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    precision_at_best = precision_score(y_test, (y_prob >= best_threshold).astype(int))\n",
    "    recall_at_best = recall_score(y_test, (y_prob >= best_threshold).astype(int))\n",
    "\n",
    "    # Accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Maximal significance calculation\n",
    "    signal_count = np.sum(y_pred == 1)\n",
    "    background_count = np.sum(y_pred == 0)\n",
    "    max_significance = signal_count / np.sqrt(signal_count + background_count)\n",
    "\n",
    "    return {\n",
    "        'TPR': tpr,\n",
    "        'FPR': fpr,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'Precision': precision_at_best,\n",
    "        'Recall': recall_at_best,\n",
    "        'F1 Score': best_f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Max Significance': max_significance\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "\n",
    "metrics = compute_classifier_metrics(dt_model, X_test, y_test)\n",
    "\n",
    "results_df = pd.DataFrame([metrics])\n",
    "\n",
    "print(tabulate(results_df, headers='keys', tablefmt='pretty'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
